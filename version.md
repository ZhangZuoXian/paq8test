这个版本是单次预训练的版本，命令行有两个参数，第一个参数是训练数据占总数据的百分比(0~100)，第二个参数是待压缩的文件
注意，这个版本虽然写了分块相关的东西，但是没用到，整体还是无分块的
把无用的时间统计备份给删了，这部分占到近一半的CPU时间

decompress:
在pre-training的基础上添加解压部分

dev:
decompress的dev分支
压缩命令格式：paq8test [分块大小] [文件名] [训练量]
解压命令格式：paq8test -d [文件名]

2023.3.1
重新写这个部分
以前：训练完成后将参数保存，静态压缩时重新生成预测器并读取参数，在静态压缩的过程中还在更新参数。该方案没有真正的静态压缩，只是在压缩每一个数据块时重置了参数。
现在：
1. 首先动态压缩更新参数，达到训练量后将整个预测器的状态和混合器中的参数固定，不再更新。静态压缩时不新生成预测器，直接使用静态的预测器。需要重训练时重置整个预测器和混合器的状态。
2. 添加多文件输入
3. 添加解压部分

TODO：
1. 动态压缩和静态压缩的区别只有一个updateState，除此之外应该都一样
2. 每压缩完一个块都应该保存块压缩前、压缩后的长度，前者用于验证解压的正确性，后者用于解压时判断终止
3. 重训练时应当完全reset，包括share、映射表、混合器
4. 能够处理混合数据，即压缩时将多个文件打包~~，解压时在合适的时候开始下一个文件~~（解压没必要做，做压缩只是为了获得测试数据说明重训练的必要性，解压直接用paq8px的就行了）（暂时不管多文件输入，先做单文件，反正最后一起改了）
5. 设置合理的输出控制，在合理的时候输出每一块的压缩信息

需要注意的问题：
1. 最后一个数据块可能不满，这个数据块可以是动态的也可以是静态的
2. 动态压缩时将全部数据认作是一个数据块，所以虽然指定了数据块的大小，在实际压缩、解压时数据块还是可能是不定长的。同时，末尾的数据块也有可能不是指定长度

2023.3.23
实现了单文件的单次训练压缩、解压。
压缩命令格式：paq8test [分块大小] [文件名] [训练量]
解压命令格式：paq8test -d [文件名]

2023.3.28
使用一个额外的程序实现了混合文件输入。test的merge用于将多个文件连接成一个文件，命令格式"./merge file1 file2 ..."，输出文件的名字由输入文件的首字母按序组成，输出到data文件夹。

2023.4.12
实现了重训练，改命令行为：
压缩命令格式：paq8test [分块大小] [文件名] [训练量] [k] [θ]
解压命令格式：paq8test -d [文件名]
其中，训练量和k为浮点数，表示百分之x
其中，k和θ是重训练触发条件中的参数，当n*k%（n是总数据块数，k是输入的值）个块静态压缩的压缩比小于训练数据压缩比的θ时进行重训练。参考值：k同训练量，θ为0.8。注意，这里的θ最多只能为两位小数，过长的小数部分会被截断。

当前的问题：
之前一直是通过delete encoder、new一个新的encoder实现参数清零的，但发现程序里使用了static，所以实际上参数没有被清空，这导致了之前的分块测试的压缩比是错误的。

2023.4.18
做如下规划：
1. 检查映射表的更新控制，做到静态压缩时映射表的时间不更新
2. 实现各个模型的reset，当reset时要把上下文清空
3. 实现混合器的reset和更新控制
4. 修改model的接口，使用一个model时模型都是一个，但可以通过一定的指令如model的reset实现映射表和模型的初始化
5. encoder的reset和模型的reset同步
6. 静态的参数应当放到堆中，为多线程做准备。这部分的参数包括映射表和混合器的数据

实现后的功能：
1. 映射表、混合器参数动态更新、静态的控制
2. 静态压缩每一个数据块后，把上下文信息（即非静态的数据）清空
3. 重训练时更新所有参数

2023.4.20
对上述的规划做出补充：按模型的顺序，一个一个解决更新控制、reset的问题，同时要区分模型的reset，是只reset上下文信息，还是需要reset映射表的信息

2023.5.15
再次确定应该逐比特更新的参数和可以自适应更新的参数。所有参数都要设置reset以便于在重训练时reset，部分参数需要blockReset，在压缩一个数据块后就reset（比如上下文信息）
对模型的所有参数进行抽象，认为部分参数是从上下文中抽取的、用于计算索引的，这部分必须逐比特更新。其他参数都可以自适应更新。
对share，所有参数都应当逐比特更新
**特别注意bucket的find函数会导致bucket中的条目被替换！这也是参数更新**

2023.5.15
现在开始检查reset函数是否包含了所有的参数，并实现自适应更新参数的更新控制
SparseModel - contextmap - statemap - adaptivemap done
NormalModel - contextmap2 done
MatchModel - indirectcontext - stataionarymap - smallstationmap done

2023.5.18
关于indirectContext到底应该逐比特更新还是自适应更新的问题。
indirectContext本质上是将输入数据看成比特流，获得当前比特的上下文hash值。
目前认为把它当成映射表信息、让其自适应更新较好，后面也可以对比一下当成上下文信息、逐比特更新。

2023.5.18
进一步明确重训练时的操作。
动态训练：updatestate为true，所有参数正常的逐比特更新。结束后blockReset。
静态压缩：上下文参数逐比特更新，其他参数停止更新。压缩一个数据块后blockReset
重训练：所有参数全部reset
逐比特更新的上下文参数包括：normalModel中的cxt即上下文的hash值；matchmodel中length、index及二者的bak，mismatch的标志delta；stationaryMap和smallstationaryMap中的b（和上下文hash一起用于索引对应的概率）；混合器中的参数
自适应更新的参数包括：contextmap、contextmap2中的bucket；~~indirectContext的data和ctx~~；statemap和adaptivemap的t；stationaryMap和smallstationaryMap中的data

TODO：mixer和shared的参数控制和reset
一点点小忧虑：重训练发生reset时可能需要进行大量的参数计算和读写，静态参数带来的时间节省真的能大过重训练的开销吗？

2023.5.25
决定混合器中的参数采用逐比特更新的方式，不再自适应更新。原因在于我不了解其内部结构，后续了解的话可以对比看看。
其实想来混合器本来就不能用静态参数，否则动态模型选择咋整？

2023.10.6
重新梳理：现在把方案分成了两个点：自适应和解压优化。自适应就是自适应参数更新和自适应模型选择，解压优化就是重训练。
给解压优化想了一个新的动机，就是一堆小数据或者小文件，我们想把他们打包在一起去压缩，这样能够提高压缩比。但这里存在两个问题：一是上下文混合压缩中压缩比和数据量不一定成正相关；二是打包的越大，我们读取的时候解压时延就越高。所以我们需要权衡后确定一个较为合适的打包大小，这里提出以压缩比为判定依据，设置上下限的方式。场景可以是增量备份的场景：增量备份后把本次备份剩余的增量全都打包到一起，如果某个文件损坏了，那就得把所有压缩包全部解开，才能依次获得增量和base文件。如果用来这个方案，就可以每个压缩包只解压部分数据
这种思路下面，自适应参数更新其实就是做分块统计当前压缩比、根据压缩比控制参数更新、做并行压缩的

2024.2.20
* 最大level貌似是12，根据关键词“level”在github的changelog中找到的